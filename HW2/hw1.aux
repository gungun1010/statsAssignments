\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem 1}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}closed form of $\mathaccentV {hat}05E\beta $}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Find a simple expression for $\delimiter "026B30D  \mathaccentV {hat}05E\beta - \beta ^* \delimiter "026B30D $}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Find a closed form of $\mathaccentV {hat}05Ef$}{1}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}implement the solution of $\mathaccentV {hat}05Ef$ in matlab}{2}{subsection.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Red circle indicates labels, blue cross is the $\mathaccentV {hat}05Ef$ value. $n=10, \lambda =1, \sigma ^2 = 0.1$ in this test case. I see that when $x^{(i)}$ is close to decision boundary, 0, it is more accurate in this setting.It is an underfit case. }}{3}{figure.1}}
\newlabel{problem1Pic1}{{1}{3}{Red circle indicates labels, blue cross is the $\hat f$ value. $n=10, \lambda =1, \sigma ^2 = 0.1$ in this test case. I see that when $x^{(i)}$ is close to decision boundary, 0, it is more accurate in this setting.It is an underfit case}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  $\lambda =1, \sigma = 1$ in this test case. I see that most of the $x^{(i)}$ can be predicted accurately with two exceptions at the end of the decision boundary }}{3}{figure.2}}
\newlabel{problem1Pic2}{{2}{3}{$\lambda =1, \sigma = 1$ in this test case. I see that most of the $x^{(i)}$ can be predicted accurately with two exceptions at the end of the decision boundary}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  $\lambda =1, \sigma = 5$ in this test case. Though the $x^{(i)}$ at both ends are better predicted, the average accuracy has dropped. }}{4}{figure.3}}
\newlabel{problem1Pic2}{{3}{4}{$\lambda =1, \sigma = 5$ in this test case. Though the $x^{(i)}$ at both ends are better predicted, the average accuracy has dropped}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  $\lambda =1, \sigma = 10$. All $x^{(i)}$ are fitted badly. It is clearly a case of over fitting. }}{4}{figure.4}}
\newlabel{problem1Pic2}{{4}{4}{$\lambda =1, \sigma = 10$. All $x^{(i)}$ are fitted badly. It is clearly a case of over fitting}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  $\lambda =1, \sigma = 0.1$. It is clearly a case of under fit. It is way under fit so that the decision boundary looks like a straight line. }}{5}{figure.5}}
\newlabel{problem1Pic2}{{5}{5}{$\lambda =1, \sigma = 0.1$. It is clearly a case of under fit. It is way under fit so that the decision boundary looks like a straight line}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  $\lambda =0.01, \sigma = 3$. Most of the predictions for $x^{(i)}$ tends to overlap with their labels or be really close to their actual labels. I consider this is a good fit for the test data. }}{5}{figure.6}}
\newlabel{problem1Pic2}{{6}{5}{$\lambda =0.01, \sigma = 3$. Most of the predictions for $x^{(i)}$ tends to overlap with their labels or be really close to their actual labels. I consider this is a good fit for the test data}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem 2}{6}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Heatmap of learned function}{6}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  $\sigma = 1$. This heatmap is for training dataset,we can clearly see the decision boundary based on the heatmap}}{6}{figure.7}}
\newlabel{problem2Pic1}{{7}{6}{$\sigma = 1$. This heatmap is for training dataset,we can clearly see the decision boundary based on the heatmap}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Level curves}{7}{subsection.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  When $\sigma = 5$. we can not see the boundary }}{7}{figure.8}}
\newlabel{problem2Pic2}{{8}{7}{When $\sigma = 5$. we can not see the boundary}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  When $\sigma = 1$. we can see the boundary but the heatmap is still quite vague}}{7}{figure.9}}
\newlabel{problem2Pic3}{{9}{7}{When $\sigma = 1$. we can see the boundary but the heatmap is still quite vague}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  When $\sigma = 0.1$. The boundary became clear and the decision values are most accurate}}{8}{figure.10}}
\newlabel{problem2Pic4}{{10}{8}{When $\sigma = 0.1$. The boundary became clear and the decision values are most accurate}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces  When $\sigma = 0.02$. The boundary fades away and is about to disappear}}{8}{figure.11}}
\newlabel{problem2Pic5}{{11}{8}{When $\sigma = 0.02$. The boundary fades away and is about to disappear}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces  When $\sigma = 0.01$. The boundary is again gone}}{9}{figure.12}}
\newlabel{problem2Pic6}{{12}{9}{When $\sigma = 0.01$. The boundary is again gone}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  $\sigma = 1$ level curve of $\mathaccentV {hat}05Ef = 0$ }}{9}{figure.13}}
\newlabel{problem2Pic6}{{13}{9}{$\sigma = 1$ level curve of $\hat f = 0$}{figure.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces  $\sigma = 0.01$ level curve of $\mathaccentV {hat}05Ef = 0$ }}{10}{figure.14}}
\newlabel{problem2Pic6}{{14}{10}{$\sigma = 0.01$ level curve of $\hat f = 0$}{figure.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Plot the training and testing error vs $1/\sigma $}{10}{subsection.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces  $\sigma \in [1,0.01)$ with step size -0.02. X-axis shows $1/\sigma $, and y-axis shows the accuracy in $\%$, we see that even though the error keeps reducing with smaller $\sigma $ value, the testing results is not improving. This is because while $\sigma \rightarrow 0$, the decision boundary is overfitted onto each data point, ${x_i}_{i=1}^n, n = 10,000$. The testing dataset cannot benefit from the overfitting of training dataset.}}{11}{figure.15}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem 3}{11}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Show $k(x,y)$ is a valid kernel}{11}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Prove RKHS}{12}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Problem 4}{12}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  mean square error between $\mathaccentV {hat}05Ef and f$ plot with the testing dataset. $n$ is set from 100 to 2000 with step size of 20. The plot shows that it is when n is very small, the error is minimum. The error reachs a maximum at halfway point and declines afterwards. }}{12}{figure.16}}
\newlabel{problem2Pic6}{{16}{12}{mean square error between $\hat f and f$ plot with the testing dataset. $n$ is set from 100 to 2000 with step size of 20. The plot shows that it is when n is very small, the error is minimum. The error reachs a maximum at halfway point and declines afterwards}{figure.16}{}}
