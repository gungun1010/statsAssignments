\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Problem 1}{1}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}closed form of $\mathaccentV {hat}05E\beta $}{1}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Find a simple expression for $\delimiter "026B30D  \mathaccentV {hat}05E\beta - \beta ^* \delimiter "026B30D $}{1}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Find a closed form of $\mathaccentV {hat}05Ef$}{1}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}implement the solution of $\mathaccentV {hat}05Ef$ in matlab}{2}{subsection.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Red circle indicates labels, blue cross is the $\mathaccentV {hat}05Ef$ value. $n=10, \lambda =1, \sigma ^2 = 0.1$ in this test case. I see that when $x^{(i)}$ is close to decision boundary, 0, it is more accurate in this setting.It is an underfit case. }}{3}{figure.1}}
\newlabel{problem1Pic1}{{1}{3}{Red circle indicates labels, blue cross is the $\hat f$ value. $n=10, \lambda =1, \sigma ^2 = 0.1$ in this test case. I see that when $x^{(i)}$ is close to decision boundary, 0, it is more accurate in this setting.It is an underfit case}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  $\lambda =1, \sigma = 1$ in this test case. I see that most of the $x^{(i)}$ can be predicted accurately with two exceptions at the end of the decision boundary }}{3}{figure.2}}
\newlabel{problem1Pic2}{{2}{3}{$\lambda =1, \sigma = 1$ in this test case. I see that most of the $x^{(i)}$ can be predicted accurately with two exceptions at the end of the decision boundary}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  $\lambda =1, \sigma = 5$ in this test case. Though the $x^{(i)}$ at both ends are better predicted, the average accuracy has dropped. }}{4}{figure.3}}
\newlabel{problem1Pic2}{{3}{4}{$\lambda =1, \sigma = 5$ in this test case. Though the $x^{(i)}$ at both ends are better predicted, the average accuracy has dropped}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  $\lambda =1, \sigma = 10$. All $x^{(i)}$ are fitted badly. It is clearly a case of over fitting. }}{4}{figure.4}}
\newlabel{problem1Pic2}{{4}{4}{$\lambda =1, \sigma = 10$. All $x^{(i)}$ are fitted badly. It is clearly a case of over fitting}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  $\lambda =1, \sigma = 0.1$. It is clearly a case of under fit. It is way under fit so that the decision boundary looks like a straight line. }}{5}{figure.5}}
\newlabel{problem1Pic2}{{5}{5}{$\lambda =1, \sigma = 0.1$. It is clearly a case of under fit. It is way under fit so that the decision boundary looks like a straight line}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  $\lambda =0.01, \sigma = 3$. Most of the predictions for $x^{(i)}$ tends to overlap with their labels or be really close to their actual labels. I consider this is a good fit for the test data. }}{5}{figure.6}}
\newlabel{problem1Pic2}{{6}{5}{$\lambda =0.01, \sigma = 3$. Most of the predictions for $x^{(i)}$ tends to overlap with their labels or be really close to their actual labels. I consider this is a good fit for the test data}{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Problem 2}{6}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Heatmap of learned function}{6}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  $\sigma = 1$. This heatmap is for training dataset,we can clearly see the decision boundary based on the heatmap}}{6}{figure.7}}
\newlabel{problem1Pic2}{{7}{6}{$\sigma = 1$. This heatmap is for training dataset,we can clearly see the decision boundary based on the heatmap}{figure.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Problem 3}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Show $k(x,y)$ is a valid kernel}{6}{subsection.3.1}}
